在这项工作中，我们研究分布式模型中的差分隐私，包括local和shuffle设置。通过矩匹配方法和新定义的确定性协议概念，我们对于这两个模型中的三个基本问题Count-Distinct、Selection和Parity-Learning给出了新的下界。虽然我们给出的下限在大多数的参数设置下是紧的，但仍留有许多有趣的开放问题。以下列举三个我们重点强调问题：
消息数量无界协议的DP-shuffle下界。当k→∞时，DP-shuffle与确定性协议之间的联系变得更弱。这导致它不能用于建立消息数量不受限制时的DP-shuffle协议的下限。实际上，在没有消息数量与鲁棒性限制的条件下，我们还无法得知中心化DP和DP-shuffle之间的分隔。这仍然是一个基本的公开问题。（相比之下，中心化DP与DP-local之间的分隔已被充分地了解，甚至包括二进制求和等基本函数）

In this work, we study DP in distributed models, including the local and shuffle settings. By building on the moment matching method and using the newly defined notion of dominated protocols, we give novel lower bounds in both models for three fundamental problems: Count-Distinct, Selection and Parity-Learning. While our lower bounds are nearly tight in a large setting of parameters, there are still many interesting open questions, three of which we highlight below:
DP-shuffle Lower Bounds for Protocols with Unbounded Number of Messages. Our connection between DP shuffle and dominated protocols becomes weaker as k → ∞. As a result, it cannot be used to establish lower bounds against DP-shuffle protocols with a possibly unbounded number of messages. In fact, we are not aware of any separation between central DP and DP-shuffle without a restriction on the number of messages and without the robustness restriction. This remains a fundamental open question. (In contrast, separations between central DP and DP-local are well-known, even for basic functions such as binary summation.)
