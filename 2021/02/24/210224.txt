对于用户数据的隐私需求对与组织和用户呈现着相似的增长趋势。过去几年中发生了多次大规模的隐私泄露事件，显示出当今大多数基础架构的危急性和脆弱性。尤其是关于可信数据管理者的概念存在争议，数据管理者接收用户发送的原始数据，并使用这些数据为针对性广告等不同任务构建模型。正如Kearns和Roth在他们最近关于算法道德的书中所说的那样，“为了确保这些模型的影响尊重我们要维护的社会规范，我们需要学习如何将这些目标直接地设计到我们的算法中”。为了实现这一目标，本文研究了如何将隐式地将隐私纳入相似搜索系统。

Privacy of user data is becoming an ever increasing need for organizations and users alike. Multiple large-scale privacy breaches in the last years showed how critical and vulnerable most of today's infrastructure is. In particular, there is dispute about the concept of a trusted data curator to whom users send their original data, and who uses this data to build models for different tasks such as targeted advertisement. As Kearns and Roth put it in their recent book about ethical algorithms, "to make sure that the effect of these models respect the societal norms that we want to maintain, we need to learn how to design these goals directly into our algorithms." In pursue of this goal, the present paper studies how we can implicitly incorporate privacy into a similarity search system.
