现有的联邦学习方法存在通信瓶颈问题和由于稀疏客户端参与引起的收敛问题。本文介绍了一种新颖的算法来解决这些挑战，称为FetchSGD。FetchSGD使用一个Count Sketch压缩模型的更新，并且利用sketch的可合并性将不同来源的模型更新进行组合。FetchSGD设计上的关键点在于Count Sketch是线性的，因此动量和误差累加都可以在sketch内完成。这使算法可以将动量和错误累加从客户端转移到中心聚合器，克服了稀疏客户端参与的挑战，同时仍然实现了高压缩率和良好的收敛性。我们证明了FetchSGD具有良好的收敛性保证，并且通过训练两个残差网络和一个transformer模型来证明其经验有效性。

Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation. In this paper we introduce a novel algorithm, called FetchSGD, to overcome these challenges. FetchSGD compresses model updates using a Count Sketch, and then takes advantage of the mergeability of sketches to combine model updates from many workers. A key insight in the design of FetchSGD is that, because the Count Sketch is linear, momentum and error accumulation can both be carried out within the sketch. This allows the algorithm to move momentum and error accumulation from clients to the central aggregator, overcoming the challenges of sparse client participation while still achieving high compression rates and good convergence. We prove that FetchSGD has favorable convergence guarantees, and we demonstrate its empirical effectiveness by training two residual networks and a transformer model.
