当在联邦设定下训练机器学习模型时，参与的客户端不发送它们的本地数据给中心服务器；而是由一个中心聚合器协调客户端之间的优化程序。在这个程序的每一轮迭代中，客户端使用本地数据对当前模型计算基于梯度的更新，并只向中心聚合器传达这些更新。

When training machine learning models in the federated setting, participating clients do not send their local data to a central server; instead, a central aggregator coordinates an optimization procedure among the clients. At each iteration of this procedure, clients compute gradient-based updates to the current model using their local data, and they communicate only these updates to a central aggregator.
